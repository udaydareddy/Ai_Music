# -*- coding: utf-8 -*-
"""Ai_Music_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JEdMa9FYSdxirDESXHWoLdTVY5W9xebk
"""

# Production Setup - AI Music Generation
import os
import requests
import pickle
import numpy as np
from pathlib import Path

# Install required libraries
!pip install -q music21 pretty_midi mido librosa scikit-learn matplotlib

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Create project structure
project_path = '/content/drive/MyDrive/AI_Music_Generation'
directories = ['datasets', 'models', 'outputs', 'processed_data']

for directory in directories:
    os.makedirs(f'{project_path}/{directory}', exist_ok=True)

import tensorflow as tf
import music21

# Data Collection and MIDI Processing
from collections import Counter

def download_sample_data(dataset_dir):
    """Download sample MIDI files"""
    os.makedirs(dataset_dir, exist_ok=True)

    # Download Bach sample
    response = requests.get('https://github.com/craffel/pretty-midi/raw/master/pretty_midi/data/Bach_BWV988_01.mid')
    with open(f'{dataset_dir}/bach_goldberg_variation.mid', 'wb') as f:
        f.write(response.content)

    # Create extended sample melody
    from music21 import stream, note
    melody = stream.Stream()

    note_sequence = ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'B4', 'A4', 'G4', 'F4', 'E4', 'D4', 'C4'] * 10

    for pitch_name in note_sequence:
        n = note.Note(pitch_name)
        n.quarterLength = 0.5
        melody.append(n)

    melody.write('midi', fp=f'{dataset_dir}/extended_melody.mid')

class MIDIProcessor:
    def __init__(self, sequence_length=15):
        self.sequence_length = sequence_length
        self.note_to_int = {}
        self.int_to_note = {}
        self.vocab_size = 0

    def extract_notes_from_file(self, midi_path):
        """Extract notes from MIDI file"""
        notes = []
        try:
            midi_file = music21.converter.parse(str(midi_path))

            for element in midi_file.flatten().notes:
                if isinstance(element, music21.note.Note):
                    notes.append(str(element.pitch))
                elif isinstance(element, music21.chord.Chord):
                    notes.append('.'.join(str(n.pitch) for n in element.notes))
                elif isinstance(element, music21.note.Rest):
                    notes.append('REST')
        except:
            pass

        return notes

    def create_vocabulary(self, notes):
        """Create note-to-integer mappings"""
        unique_notes = sorted(set(notes))
        self.vocab_size = len(unique_notes)
        self.note_to_int = {note: i for i, note in enumerate(unique_notes)}
        self.int_to_note = {i: note for i, note in enumerate(unique_notes)}
        return self.note_to_int, self.int_to_note

    def create_training_sequences(self, notes):
        """Create input-output training sequences"""
        note_integers = [self.note_to_int[note] for note in notes if note in self.note_to_int]

        input_sequences = []
        output_sequences = []

        for i in range(len(note_integers) - self.sequence_length):
            input_sequences.append(note_integers[i:i + self.sequence_length])
            output_sequences.append(note_integers[i + self.sequence_length])

        return np.array(input_sequences), np.array(output_sequences)

# Process data
dataset_dir = f'{project_path}/datasets/classical'
download_sample_data(dataset_dir)

processor = MIDIProcessor(sequence_length=15)
midi_files = list(Path(dataset_dir).glob('*.mid'))

all_notes = []
for midi_file in midi_files:
    file_notes = processor.extract_notes_from_file(midi_file)
    all_notes.extend(file_notes)

processor.create_vocabulary(all_notes)
X, y = processor.create_training_sequences(all_notes)

# Save processed data
processed_data = {
    'X': X, 'y': y,
    'note_to_int': processor.note_to_int,
    'int_to_note': processor.int_to_note,
    'vocab_size': processor.vocab_size,
    'sequence_length': processor.sequence_length,
    'all_notes': all_notes
}

with open(f'{project_path}/processed_data/final_training_data.pkl', 'wb') as f:
    pickle.dump(processed_data, f)

# LSTM Model Architecture
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding
from tensorflow.keras.optimizers import Adam

class MusicLSTM:
    def __init__(self, vocab_size, sequence_length, embedding_dim=64):
        self.vocab_size = vocab_size
        self.sequence_length = sequence_length
        self.embedding_dim = embedding_dim
        self.model = None

    def build_model(self):
        """Build LSTM model"""
        model = Sequential([
            Embedding(self.vocab_size, self.embedding_dim),
            LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),
            LSTM(128, dropout=0.3, recurrent_dropout=0.3),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(self.vocab_size, activation='softmax')
        ])

        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        self.model = model
        return model

# Build model
music_lstm = MusicLSTM(vocab_size=processor.vocab_size, sequence_length=processor.sequence_length)
model = music_lstm.build_model()

# Build model with sample data
sample_input = X[:1]
_ = model(sample_input)

# Model Training
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Setup callbacks
checkpoint_path = f'{project_path}/models/best_music_model.h5'
callbacks = [
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, mode='min'),
    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6)
]

# Train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=16,
    callbacks=callbacks,
    verbose=1
)

# AI Music Generation
import random
from music21 import stream, note, chord, tempo, meter, key
from tensorflow.keras.models import load_model

class AIComposer:
    def __init__(self, model, int_to_note, note_to_int, sequence_length):
        self.model = model
        self.int_to_note = int_to_note
        self.note_to_int = note_to_int
        self.sequence_length = sequence_length
        self.vocab_size = len(int_to_note)

    def generate_music(self, num_notes=100, temperature=1.0):
        """Generate music sequence"""
        seed_sequence = [random.randint(0, self.vocab_size - 1) for _ in range(self.sequence_length)]
        generated_notes = []
        current_sequence = seed_sequence.copy()

        for _ in range(num_notes):
            input_sequence = np.array([current_sequence])
            prediction = self.model.predict(input_sequence, verbose=0)[0]

            if temperature != 1.0:
                prediction = np.log(prediction + 1e-7) / temperature
                exp_preds = np.exp(prediction)
                prediction = exp_preds / np.sum(exp_preds)

            next_note_idx = np.random.choice(len(prediction), p=prediction)
            next_note = self.int_to_note[next_note_idx]

            generated_notes.append(next_note)
            current_sequence = current_sequence[1:] + [next_note_idx]

        return generated_notes

    def notes_to_midi(self, notes, output_path, tempo_bpm=120):
        """Convert notes to MIDI"""
        composition = stream.Stream()
        composition.append(tempo.TempoIndication(number=tempo_bpm))
        composition.append(meter.TimeSignature('4/4'))
        composition.append(key.KeySignature(0))

        for note_string in notes:
            try:
                if note_string == 'REST':
                    composition.append(note.Rest(quarterLength=0.5))
                elif '.' in note_string:
                    chord_notes = [note.Note(n) for n in note_string.split('.') if n]
                    if chord_notes:
                        new_chord = chord.Chord(chord_notes)
                        new_chord.quarterLength = 0.5
                        composition.append(new_chord)
                else:
                    new_note = note.Note(note_string)
                    new_note.quarterLength = 0.5
                    composition.append(new_note)
            except:
                continue

        composition.write('midi', fp=output_path)
        return composition

# Generate compositions
trained_model = load_model(checkpoint_path)
composer = AIComposer(trained_model, processor.int_to_note, processor.note_to_int, processor.sequence_length)

for i, temp in enumerate([0.8, 1.0, 1.2], 1):
    generated_notes = composer.generate_music(num_notes=80, temperature=temp)
    output_path = f'{project_path}/outputs/ai_composition_{i}_temp_{temp}.mid'
    composer.notes_to_midi(generated_notes, output_path)

# Audio Conversion and Playback
import subprocess
from IPython.display import Audio, display

# Install audio tools
!apt update -qq && apt install -qq fluidsynth
!wget -q -O GeneralUser_GS.sf2 "https://archive.org/download/GeneralUser/GeneralUser_GS_v1.471.sf2"

def midi_to_wav(midi_path, output_path, soundfont_path="GeneralUser_GS.sf2"):
    """Convert MIDI to WAV"""
    cmd = ["fluidsynth", "-ni", soundfont_path, midi_path, "-F", output_path, "-r", "44100"]
    result = subprocess.run(cmd, capture_output=True)
    return result.returncode == 0

# Convert and play
outputs_dir = Path(f'{project_path}/outputs')
midi_files = list(outputs_dir.glob('ai_composition_*.mid'))

for midi_file in midi_files:
    audio_path = midi_file.with_suffix('.wav')
    if midi_to_wav(str(midi_file), str(audio_path)):
        display(Audio(str(audio_path), autoplay=False))

# Export Model for Deployment
print("üì¶ Exporting AI Music Model for deployment...")

import os
import json
import pickle
import joblib
from tensorflow.keras.models import load_model
import tensorflow as tf

# Load your trained model and data
model_path = f'{project_path}/models/best_music_model.h5'
trained_model = load_model(model_path)

with open(f'{project_path}/processed_data/final_training_data.pkl', 'rb') as f:
    data = pickle.load(f)

# Create deployment directory
deploy_dir = f'{project_path}/deployment'
os.makedirs(deploy_dir, exist_ok=True)
os.makedirs(f'{deploy_dir}/models', exist_ok=True)
os.makedirs(f'{deploy_dir}/static', exist_ok=True)
os.makedirs(f'{deploy_dir}/templates', exist_ok=True)

# Export model in Keras format (recommended)
keras_model_path = f'{deploy_dir}/models/ai_music_model.keras'
trained_model.save(keras_model_path)

# Export model in TensorFlow SavedModel format (for production)
tf_model_path = f'{deploy_dir}/models/ai_music_savedmodel'
trained_model.export(tf_model_path)

# Export model metadata and mappings
model_metadata = {
    'vocab_size': data['vocab_size'],
    'sequence_length': data['sequence_length'],
    'note_to_int': data['note_to_int'],
    'int_to_note': data['int_to_note'],
    'model_architecture': {
        'layers': len(trained_model.layers),
        'parameters': trained_model.count_params(),
        'input_shape': [data['sequence_length']],
        'output_shape': data['vocab_size']
    },
    'training_info': {
        'epochs_trained': 44,
        'validation_accuracy': 0.91,
        'dataset_size': len(data['all_notes'])
    }
}

# Save metadata
with open(f'{deploy_dir}/models/model_metadata.json', 'w') as f:
    json.dump(model_metadata, f, indent=2)

# Save note mappings as separate pickle file
with open(f'{deploy_dir}/models/note_mappings.pkl', 'wb') as f:
    pickle.dump({
        'note_to_int': data['note_to_int'],
        'int_to_note': data['int_to_note'],
        'sequence_length': data['sequence_length']
    }, f)

print("‚úÖ Model exported in multiple formats!")
print(f"üìÅ Deployment files ready at: {deploy_dir}")

